Random Forest Model
-----------------------------------------------
from sklearn.ensemble import RandomForestRegressor
from sklearm.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random+state=1)
forest_model.fit(train_X, train_y)
preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y,preds))
---------------------------------------------------


Incosistent Data Entry
-----------------------------------------------
professors['Country'] = professors['Country'].str.lower()
professors['Country'] = professors['Country'].str.strip()

def replace_matches_in_column(df, column, string_to_match, min_ratio = 47)
  #get a list of unique strings
  strings = df[columns].unique()

  #get the top 10 closet matches to our input string
  matches = fuzzywuzzy.process.extract(string_to_match, strings, limit=10, score= fuzzywuzzy.fuzz.token_sort_ratio)

  #only get matches with a ratio > 90
  close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]

  #get the rows of all the close matches in our dataframe 
  rows_with_matches = df[column].isin(close_matches)
  
  #replace all rows with close matches with the input matches
  df.loc[rows_with_matches, column] = string_to_match
----------------------------------------------------------------
  
Approaches to clear missing values
---------------------------------------
# 1.Drop columns with missing values
cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]

reduced_X_train = X_train.drop(cols_with_missing, axis = 1)
reduced_X_valid = X_valid.drop(cols_with_missing, axis = 1)

# 2.Imputation with the mean
from sklearn.impute import SimpleImputer

my_imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))

imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns
----------------------------------------------------------------------------------

Breaking off validation set
----------------------------------------------------------------------------------
import pandas as pd
from sklearn.model_selection import train_test_split

x_full.dropna(axis=0, subset=['SalePrice', inplace=True])
y = x_full.SalePrice
x_full.drop(['SalePrice'], axis=1, inplace=True)

#drop columns with categorical data
x = x_full.select_dtypes(exclude=['object'])
x_test = x_test_full.select_dtypes(exclude=['object'])

X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)
--------------------------------------------------------------------------------------------------------------

Ordinal encoding
------------------------------------------------------------------------------
# Categorical columns in the training data
object_cols = [col for col in X_train.columns if X_train[col].dype == 'object']

#Columns that can be safely ordinal encoded
good_label_cols = [col fo col in object_cols iif set(X_valid[col]).issubset(set(X_train[col]))]

# Problematic columns that will be dropped from the dataset
bad_label_cols = list(set(object_cols)-set(good_label_cols))

from sklearn.preprocessing import OrdinalEncoder

# Drop categorical columns that will not be encoded
label_X_train = X_train.drop(bad_label_cols, axis=1)
label_X_valid = X_valid.drop(bad_label_cols, axis=1)

# Apply ordinal encoder 
X = OrdinalEncoder()
label_X_train[good_label_cols] = X.fit_transform(X_train[good_label_cols])
label_X_valid[good_label_cols] = X.transform(X_valid[good_label_cols])

# Get number of unique entries in each column with categorical data
object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))
d = dict(zip(object_cols, object_nunique))

# Print number of unique entries by column, in ascending order
sorted(d.items(), key=lambda x: x[1])
------------------------------------------------------------------

--------------------------------------------------------------------------------
# Columns that will be one-hot encoded
low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]

# Columns that will be dropped from the dataset
high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))
--------------------------------------------------------------------------------

One Hot Encoder
-----------------------------------------------------------------------------
from sklearn.preprocessing import OneHotEncoder

x = OneHotEncoder(handle_unknown = "ignore",sparse=False)
OH_cols_train = pd.DataFrame(x.fit_transform(X_train[low_cardinality_cols])) 
OH_cols_valid = pd.DataFrame(x.transform(X_valid[low_cardinality_cols])) 

OH_cols_train.index = X_train.index
OH_cols_valid.index = X_valid.index

num_train = X_train.drop(object_cols,axis = 1)
num_valid = X_valid.drop(object_cols,axis = 1)

OH_X_train = pd.concat([OH_cols_train, num_train],axis =1)
OH_X_valid = pd.concat([OH_cols_valid, num_valid],axis =1)
---------------------------------------------------------------------------------
